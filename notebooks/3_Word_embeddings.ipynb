{"cells":[{"cell_type":"code","execution_count":null,"id":"5dd75408","metadata":{"id":"5dd75408"},"outputs":[],"source":["#import required libraries \n","import gensim\n","\n","from gensim.models import Word2Vec\n","import gensim.downloader as api\n","import pandas as pd \n","import re\n","import numpy as np "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g391BQNos2vF","executionInfo":{"status":"ok","timestamp":1643881732016,"user_tz":-60,"elapsed":1988,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"6c9308c2-b9cf-4627-f0f8-bdd100bf2c84"},"id":"g391BQNos2vF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"id":"11b74769","metadata":{"id":"11b74769"},"outputs":[],"source":["#I report here again the tokanization function because we will need them in the cells below\n","def sent_tokenizer(text):\n","    return re.findall(r\".*?[.!\\?]\",text)\n","\n","def word_tokenizer(sentence):\n","    punct = r\"\"\"([A-z])([,;:\\?!\\.\"'])\"\"\"\n","    temp_sentence =  re.sub(punct, r\"\\1 \\2\", sentence)\n","    toks = temp_sentence.split()\n","    temp_out =[]\n","    # splitting english possessive\n","    for tok in toks:\n","        if re.search(r\"([A-z]+)’s?$\", tok):\n","            temp_out.extend(re.sub(r\"([A-z]+)(’s?)$\", r\"\\1 \\2\", tok).split())\n","        else:\n","            temp_out.append(tok)\n","    return temp_out \n","\n","def my_tokenizer(text):\n","    import string \n","    punct = string.punctuation \n","    sentences = sent_tokenizer(text)\n","    tokenized_text = []\n","    for sent in sentences:\n","        if len(sent) > 1 and sent != []:\n","            tokens = word_tokenizer(sent)\n","            tokens_1 = [tok for tok in tokens if not tok in punct]\n","            tokenized_text.append(tokens_1)\n","    return tokenized_text"]},{"cell_type":"code","execution_count":null,"id":"5f8b5dfc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f8b5dfc","executionInfo":{"status":"ok","timestamp":1643881947457,"user_tz":-60,"elapsed":30673,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"7c01a39b-2c17-4dba-c4b2-0e58ade721b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["#open the two final experimental corpora\n","depression = pd.read_csv('/content/drive/MyDrive/Computational_Linguistics_Project /datasets/depression_finale.csv', index_col= False)\n","suicide = pd.read_csv('/content/drive/MyDrive/Computational_Linguistics_Project /datasets/suicidio_finale.csv', index_col= False)"]},{"cell_type":"code","execution_count":null,"id":"c88a7a20","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c88a7a20","executionInfo":{"status":"ok","timestamp":1643881997836,"user_tz":-60,"elapsed":512,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"47e29a70-bc9d-4c77-f7a5-d8f3214c2c42"},"outputs":[{"output_type":"stream","name":"stdout","text":["(12059, 4)\n"]}],"source":["#open the control corpus\n","corpus_cont = pd.read_csv('/content/drive/MyDrive/Computational_Linguistics_Project /datasets/FINALE_controllo.csv', index_col= False)\n","print(corpus_cont.shape)"]},{"cell_type":"code","execution_count":null,"id":"391ea58d","metadata":{"id":"391ea58d"},"outputs":[],"source":["# gold standard model to compare with our word embeddings\n","MEN = [line.split(\" \") for line in open(\"/content/drive/MyDrive/Computational_Linguistics_Project /word_embedding_models/MEN/MEN_dataset_natural_form_full\").read().split(\"\\n\")[:-1]]\n","MEN = [(t1,t2,float(sim)) for (t1,t2,sim) in MEN]"]},{"cell_type":"code","execution_count":null,"id":"6647ffa7","metadata":{"id":"6647ffa7"},"outputs":[],"source":["#one more passage to ensure that we have strings to process\n","deprex = [str(ele) for ele in depression['cleaned_text'][:]]\n","suix = [str(ele) for ele in suicide['cleaned_text'][:]]\n","not_illness = [str(ele) for ele in corpus_cont['cleaned_text'][:]]"]},{"cell_type":"code","execution_count":null,"id":"34320130","metadata":{"id":"34320130"},"outputs":[],"source":["#preparation of data in the form suitable to feed our wordembedding algorithm #DEPRESSION\n","out_depre = []\n","for i,article in enumerate(deprex):\n","    tok_sent = my_tokenizer(article)\n","    for ele in tok_sent:\n","        \n","        out_depre.append(ele)"]},{"cell_type":"code","execution_count":null,"id":"4ff64b93","metadata":{"id":"4ff64b93"},"outputs":[],"source":["#preparation of data in the form suitable to feed our wordembedding algorithm #SUICIDE\n","out_sui = []\n","for i,article in enumerate(suix):\n","    tok_sent = my_tokenizer(article)\n","    for ele in tok_sent:\n","        \n","        out_sui.append(ele)\n","        "]},{"cell_type":"code","execution_count":null,"id":"89323a74","metadata":{"id":"89323a74"},"outputs":[],"source":["#preparation of data in the form suitable to feed our wordembedding algorithm #CONTROLLO\n","out_controllo = []\n","for i,article in enumerate(not_illness):\n","    tok_sent = my_tokenizer(article)\n","    for ele in tok_sent:\n","        \n","        out_controllo.append(ele)"]},{"cell_type":"markdown","id":"cdb92109","metadata":{"id":"cdb92109"},"source":["### With the following command I ask to build a word embedding with the Word2Vec algorithm\n","According to a detailed comparison of Word2Vec and fastText in this notebook, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText on semantic tasks though. The differences grow smaller as the size of the training corpus increases."]},{"cell_type":"code","execution_count":null,"id":"4469e037","metadata":{"id":"4469e037"},"outputs":[],"source":["#word_embedding D\n","depressed_model = Word2Vec(sentences= out_depre, sg=1)"]},{"cell_type":"code","execution_count":null,"id":"fcff040a","metadata":{"id":"fcff040a"},"outputs":[],"source":["#word_embedding S\n","suicide_model = Word2Vec(sentences = out_sui, sg = 1)"]},{"cell_type":"code","execution_count":null,"id":"9d278e7e","metadata":{"id":"9d278e7e"},"outputs":[],"source":["#word_embedding C\n","control_model = Word2Vec(sentences= out_controllo , sg=1)"]},{"cell_type":"code","execution_count":null,"id":"e820f93c","metadata":{"id":"e820f93c"},"outputs":[],"source":["#save the models (already_done)\n","depressed_model.save(\"depressed.model\")\n","suicide_model.save('suicide.model')\n","control_model.save('control_model')"]},{"cell_type":"code","execution_count":null,"id":"31f39c66","metadata":{"id":"31f39c66"},"outputs":[],"source":["def get_comparable_datasets(gold_dataset, dataset2):\n","    comparable_gold = []\n","    dataset2_sim = []\n","    for t1,t2,sim in gold_dataset: \n","        try: \n","            similarity = dataset2.wv.similarity(t1,t2)\n","            dataset2_sim.append((t1,t2,similarity))\n","            comparable_gold.append((t1,t2,sim))\n","        except KeyError: \n","            pass\n","    sorted_gold = sorted(comparable_gold, key = lambda x : x[2], reverse=False)\n","    sorted_dataset2 = sorted(dataset2_sim, key = lambda x : x[2], reverse=False)\n","    return sorted_gold, sorted_dataset2"]},{"cell_type":"code","execution_count":null,"id":"f409a287","metadata":{"id":"f409a287","outputId":"817fa3c1-bac3-4907-bc2d-86d94640d9d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spearman's correlation between depressed model and MEN gold dataset is:\n","\tSpearmanrResult(correlation=0.7742795450832266, pvalue=0.0)\n","Spearman's correlation between suicide model and MEN gold dataset is:\n","\tSpearmanrResult(correlation=0.7695043710734323, pvalue=0.0)\n","Spearman's correlation between control model and MEN gold dataset is:\n","\tSpearmanrResult(correlation=0.7723576087829946, pvalue=0.0)\n"]}],"source":["from scipy.stats import spearmanr\n","\n","# experimental depression\n","gold, model = get_comparable_datasets(MEN, depressed_model)\n","mental = spearmanr(gold, model, axis=None )\n","print(f\"Spearman's correlation between depressed model and MEN gold dataset is:\\n\\t{mental}\")\n","\n","\n","# experimental suicide\n","gold, model = get_comparable_datasets(MEN, suicide_model)\n","mental = spearmanr(gold, model, axis=None )\n","print(f\"Spearman's correlation between suicide model and MEN gold dataset is:\\n\\t{mental}\")\n","\n","\n","# control\n","gold2, model2 = get_comparable_datasets(MEN, control_model)\n","not_mental = spearmanr(gold2, model2, axis=None )\n","print(f\"Spearman's correlation between control model and MEN gold dataset is:\\n\\t{not_mental}\")"]},{"cell_type":"markdown","id":"dd5aafac","metadata":{"id":"dd5aafac"},"source":["### Here I manually create the clusters and try to evaluate the distance between the centroids both in the experimental and in the control group"]},{"cell_type":"code","execution_count":null,"id":"bd2b6bf7","metadata":{"id":"bd2b6bf7"},"outputs":[],"source":["#I define the cosine similarity function \n","def cosine_similarity(a, b):\n","    nominator = np.dot(a, b)\n","    \n","    a_norm = np.sqrt(np.sum(a**2))\n","    b_norm = np.sqrt(np.sum(b**2))\n","    \n","    denominator = a_norm * b_norm\n","    \n","    cosine_similarity = nominator / denominator\n","    \n","    return cosine_similarity"]},{"cell_type":"markdown","id":"17909bad","metadata":{"id":"17909bad"},"source":["#### Here I want to test the 1st hypothesis of my project: \n","In text related to depression, we expect a greater distance between first person singular pronouns and other pronouns rather than in the text coming from other subreddits (control groups)"]},{"cell_type":"code","execution_count":null,"id":"0d4174ec","metadata":{"id":"0d4174ec"},"outputs":[],"source":["#depression clusters\n","v_ME_d = depressed_model.wv['me']\n","v_I_d = depressed_model.wv['i']\n","v_SELF_d = depressed_model.wv['myself'] \n","v_MY_d = depressed_model.wv['my']\n","centroid_SELF_d = (v_ME_d + v_I_d +v_SELF_d + v_MY_d)/4\n","\n","\n","v_OTHER_d = depressed_model.wv['we']\n","v_ALL_d = depressed_model.wv['all']\n","v_TOG_d = depressed_model.wv['together']\n","v_THEY_d = depressed_model.wv['they']\n","v_YOU_d = depressed_model.wv['you']\n","v_DO_d = depressed_model.wv['us']\n","centroid_OTHER_d = (v_OTHER_d+ v_ALL_d+ v_TOG_d + v_THEY_d + v_YOU_d + v_DO_d )/6"]},{"cell_type":"code","execution_count":null,"id":"9f6d4ccf","metadata":{"id":"9f6d4ccf"},"outputs":[],"source":["#control clusters \n","v_ME_c = control_model.wv['me']\n","v_I_c = control_model.wv['i']\n","v_SELF_c = control_model.wv['self'] \n","v_MY_c = control_model.wv['my']\n","centroid_SELF_c = (v_ME_c + v_I_c +v_SELF_c + v_MY_c)/4\n","\n","\n","v_WE_c = control_model.wv['we']\n","v_ALL_c = control_model.wv['all']\n","v_TOG_c = control_model.wv['together']\n","v_THEY_c = control_model.wv['they']\n","v_YOU_c = control_model.wv['you']\n","v_US_c = control_model.wv['us']\n","centroid_OTHER_c = (v_WE_c + v_ALL_c + v_TOG_c + v_THEY_c + v_YOU_c + v_US_c)/6"]},{"cell_type":"code","execution_count":null,"id":"d640f103","metadata":{"id":"d640f103","outputId":"e8c1c2c8-1a5b-432b-815a-1bfc987dd10e"},"outputs":[{"data":{"text/plain":["0.59665304"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["similarity_depression = cosine_similarity(centroid_SELF_d,centroid_OTHER_d) #should be smaller than the one in the control group <and it's like that>\n","similarity_depression"]},{"cell_type":"code","execution_count":null,"id":"46638650","metadata":{"id":"46638650","outputId":"f65f9b31-fcd4-481c-c4a3-355c76acb056"},"outputs":[{"data":{"text/plain":["0.66783434"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["similarity_control = cosine_similarity(centroid_SELF_c,centroid_OTHER_c) \n","similarity_control"]},{"cell_type":"markdown","id":"c5e82874","metadata":{"id":"c5e82874"},"source":["#### Here I want to test the 2nd hypothesis of my project:\n","\n","In text related to suicidal ideation,we expect a smaller distance between first person singular pronouns and death/negative related works rather than in the text coming from other subreddits (control groups)"]},{"cell_type":"code","execution_count":null,"id":"3c34d6c4","metadata":{"id":"3c34d6c4"},"outputs":[],"source":["#suicide clusters\n","v_GR_s = suicide_model.wv['grave']\n","v_DE_s = suicide_model.wv['death']\n","v_DY_s = suicide_model.wv['dying']\n","v_END_s = suicide_model.wv['end']\n","v_LI_s = suicide_model.wv['final']\n","s = suicide_model.wv['exit']\n","centroid_DEATH_s = (v_GR_s + v_DE_s +v_DY_s + v_END_s + v_LI_s + s)/6\n","\n","\n","#v_ME_s = suicide_model.wv['me']\n","#v_SELF_s = suicide_model.wv['myself']\n","v_I_s = suicide_model.wv['i']\n","#v_MY_s = suicide_model.wv['my']\n","centroid_SELF_s = (v_ME_s + v_SELF_s +v_I_s + v_MY_s)/4"]},{"cell_type":"code","execution_count":null,"id":"7dd69d5f","metadata":{"id":"7dd69d5f"},"outputs":[],"source":["#control clusters \n","#centroid_SELF_c taken from above\n","v_GR_c = control_model.wv['grave']\n","v_DE_c = control_model.wv['death']\n","v_DY_c = control_model.wv['dying']\n","v_END_c = control_model.wv['end']\n","v_LI_c = control_model.wv['final']\n","centroid_DEATH_c = (v_GR_c + v_DE_c + v_DY_c + v_END_c + v_LI_c )/5"]},{"cell_type":"code","execution_count":null,"id":"3178f08b","metadata":{"id":"3178f08b","outputId":"719aba5b-90e3-4d3c-a9e9-06c74a87f60c"},"outputs":[{"data":{"text/plain":["0.3620802"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["#suicide: similarity between I and death\n","cosine_similarity(v_I_s,centroid_DEATH_s)"]},{"cell_type":"code","execution_count":null,"id":"b27561fa","metadata":{"id":"b27561fa","outputId":"cd4b8bec-474c-4d69-edbc-2236dce65802"},"outputs":[{"data":{"text/plain":["0.43599802"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["#control: similarity between I and death\n","cosine_similarity(v_I_c ,centroid_DEATH_c)"]},{"cell_type":"markdown","id":"2dd9d081","metadata":{"id":"2dd9d081"},"source":["#### I try to use a pretrained model:\n","I think it's more reliable in testing this second hypothesis. I put into practice something that originally the professor suggested me"]},{"cell_type":"code","execution_count":null,"id":"df1e085f","metadata":{"id":"df1e085f"},"outputs":[],"source":["import gensim.downloader\n","from gensim.models import KeyedVectors\n","# Show all available models in gensim-data\n","#print(list(gensim.downloader.info()['models'].keys()))\n","\n","model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Computational_Linguistics_Project /GoogleNews-vectors-negative300.bin', binary=True)"]},{"cell_type":"code","execution_count":null,"id":"8ae26aa0","metadata":{"id":"8ae26aa0"},"outputs":[],"source":["#new control clusters \n","v1_c = model['me']\n","v2_c = model['i']\n","v3_c = model['myself']\n","v4_c = model['my']\n","primo_centroid = (v1_c+ v2_c+ v3_c+ v4_c)/4\n","\n","\n","v5_c = model['grave']\n","v6_c = model['death']\n","v7_c = model['dying']\n","v8_c = model['end']\n","v9_c = model['final']\n","secondo_centroid = (v5_c + v6_c + v7_c+  v8_c + v9_c )/5"]},{"cell_type":"code","execution_count":null,"id":"39246e29","metadata":{"id":"39246e29","outputId":"7684656c-97eb-41a9-fff4-09fab4c6d83b"},"outputs":[{"data":{"text/plain":["0.07756988"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["#control: similarity between I and death\n","cosine_similarity(v2_c ,secondo_centroid)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"3_Word_embeddings.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}