{"cells":[{"cell_type":"markdown","id":"e80f5e6d","metadata":{"id":"e80f5e6d"},"source":["### Now we have both our final datasets and we have to start the pre-processing phase\n","The two datasets are a bit unbalanced, for this reason we should collect more data for the control one. Let's see later how to do that. "]},{"cell_type":"code","execution_count":null,"id":"a0a0c272","metadata":{"id":"a0a0c272"},"outputs":[],"source":["#import required libraries \n","import csv\n","import json\n","import os\n","import requests  \n","import time\n","import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import re\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from zipfile import ZipFile\n","from nltk import sent_tokenize\n","from nltk import word_tokenize"]},{"cell_type":"code","execution_count":null,"id":"395ed17a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"395ed17a","executionInfo":{"status":"ok","timestamp":1643877038087,"user_tz":-60,"elapsed":22780,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"1562778c-cd86-42f3-aa5a-ca571a454495"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["#open experimental group dataset\n","experimental_df_1 = pd.read_csv('/content/drive/MyDrive/Computational_Linguistics_Project /datasets/Cleaned_Depression_Vs_Suicide.csv')\n","experimental_df_1 = experimental_df_1.reset_index(drop=True)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTrN3pIIqi9K","executionInfo":{"status":"ok","timestamp":1643876892486,"user_tz":-60,"elapsed":25174,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"37e94f02-d239-4a4c-851e-445d72fc14d3"},"id":"MTrN3pIIqi9K","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"29cfcac5","metadata":{"id":"29cfcac5"},"outputs":[],"source":["#dropping duplicates\n","experimental_df_1 = experimental_df_1.drop_duplicates(subset=['text'])\n","experimental_df_1 = experimental_df_1.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"id":"fbafa758","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbafa758","executionInfo":{"status":"ok","timestamp":1643877055463,"user_tz":-60,"elapsed":689,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"41a1baa6-48f8-441b-888e-148e58de9cd1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["609770"]},"metadata":{},"execution_count":7}],"source":["#Dropping rows with NaN values\n","experimental_df_1.dropna(axis = 0, inplace = True)\n","len(experimental_df_1)"]},{"cell_type":"code","execution_count":null,"id":"913705f7","metadata":{"id":"913705f7"},"outputs":[],"source":["cleaned_intero_tozip = experimental_df_1.copy()"]},{"cell_type":"markdown","id":"13b2ff7e","metadata":{"id":"13b2ff7e"},"source":["NB: the posts ***non-suicide*** and ***depression***, are the same  come from the same subreddit channel. You can easily filter for them in order to obtain the single word embedding models. "]},{"cell_type":"code","execution_count":null,"id":"66887056","metadata":{"id":"66887056"},"outputs":[],"source":["#open control group dataset \n","control_df_1 = pd.read_csv('/content/drive/MyDrive/Computational_Linguistics_Project /datasets/df_control_group.csv', index_col= 0)"]},{"cell_type":"code","execution_count":null,"id":"8f7571f0","metadata":{"id":"8f7571f0"},"outputs":[],"source":["#remove duplicates \n","control_df_1 = control_df_1.drop_duplicates(subset=['text'])\n","control_df_1 = control_df_1.reset_index(drop=True)\n","control_df_1 = control_df_1.dropna()\n"]},{"cell_type":"code","execution_count":null,"id":"7d157d31","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d157d31","executionInfo":{"status":"ok","timestamp":1643877083970,"user_tz":-60,"elapsed":230,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"09054c66-7e02-4060-e3c2-4af63ee8c386"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["12058"]},"metadata":{},"execution_count":12}],"source":["len(control_df_1)"]},{"cell_type":"markdown","id":"a7429083","metadata":{"id":"a7429083"},"source":["## CLEANING TEXT  of both datasets"]},{"cell_type":"code","execution_count":null,"id":"4379eb14","metadata":{"id":"4379eb14"},"outputs":[],"source":["#list of stopwords that we do not want to be removed because they are interesting for our analysis\n","do_not_remove = ['do','me', 'my', 'i', 'myself', 'we', 'our', 'you', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'her', 'hers', 'herself', 'them', 'their', 'themselves', 'up', 'down', 'ourselves', 'ours']"]},{"cell_type":"code","execution_count":null,"id":"9a5bd76d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a5bd76d","executionInfo":{"status":"ok","timestamp":1643877234895,"user_tz":-60,"elapsed":239,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"791164e7-65d1-4a31-a0d3-d80fcadfa53c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["nltk.download('stopwords')\n","stop_words = stopwords.words(\"english\")\n","#extend list of stopwords according to an arbitrary decision that I made\n","stop_words.extend(['it', 'is'])"]},{"cell_type":"code","execution_count":null,"id":"e950a642","metadata":{"id":"e950a642"},"outputs":[],"source":["for word in do_not_remove:\n","    stop_words.remove(word)"]},{"cell_type":"markdown","id":"85138540","metadata":{"id":"85138540"},"source":["### Here I defined the function which contains all the elements for cleaning the Reddit text."]},{"cell_type":"code","execution_count":null,"id":"e96a3bf8","metadata":{"id":"e96a3bf8"},"outputs":[],"source":["import re\n","\n","def clean_up(testo):\n","    punctu = '\"#$%&\\'()*+-/:;<=>@[\\\\]^_`{|}~'\n","    testo = str(testo)\n","    testo = testo.lower()\n","    testo = re.sub(r\"(i'm)\", 'i am', testo)\n","    testo = re.sub(r'ama', 'i am a', testo)\n","    testo = re.sub(r'iama', 'i am a', testo)\n","    testo = re.sub(r\"(i've)\", 'i have', testo)\n","    testo = re.sub(r\"i'm\", 'i am', testo )\n","    testo = re.sub(r\"i've\", 'i have', testo)\n","    testo = re.sub(r\"haven't'\", 'have not', testo)\n","    testo = re.sub(r'(?<=[.,?])(?=[^\\s])', r' ', testo) \n","    testo = ' '.join([word for word in testo.split(' ') if word not in stop_words]) #remove stopwords\n","    testo = re.sub(r'\\'\\w+', '', testo)  #remove ticks and the next character\n","    testo = testo.encode('ascii', 'ignore').decode()\n","    testo = re.sub(r'https*\\S+', ' ', testo) #remove hyperlinks\n","    testo = re.sub(r'@\\S+', ' ', testo) #remove mentions\n","    testo = re.sub(r'#\\S+', ' ', testo) #remove hastags\n","    testo = re.sub('[%s]' % re.escape(punctu), ' ', testo) #remove punctuation (except .,?! that we will need to sent tokenize later)\n","    testo = re.sub(r'\\w*\\d+\\w*', '', testo) #remove numbers\n","    testo = re.sub(r'\\s{2,}', ' ', testo)   #remove extra spaces\n","    return testo\n","    "]},{"cell_type":"code","execution_count":null,"id":"006856c9","metadata":{"id":"006856c9"},"outputs":[],"source":["cleaned_intero_tozip['cleaned_text'] = cleaned_intero_tozip.text.apply(clean_up)"]},{"cell_type":"code","execution_count":null,"id":"42b77676","metadata":{"id":"42b77676","outputId":"9f27b62c-5ce8-48eb-a55e-55937b8b84b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["mood changes within space couple minutes normal? ? also, pick me up suggestions like music, activities etc help me up? hi guys, basically i emotional wreck. diagnosed severe derpression my first post regarding matter. suffering years sought help things took turn worst.\n","i know supportive generally reddit could really use pick me up my virtual peers nothing else.\n","i one question thats really bugging me is, normal mood deteriorate withing space minutes trigger point hit even without it?\n","its really bugging me i thought id ask. tbh help pic me ups welcome, shit really ruining aspects my personal life making seem worse\n","cheers guys\n"]}],"source":["print(cleaned_intero_tozip.iloc[40]['cleaned_text'])\n","#clean_up(cleaned_intero_tozip.iloc[10]['text']))"]},{"cell_type":"code","execution_count":null,"id":"35ce2fb5","metadata":{"id":"35ce2fb5"},"outputs":[],"source":["control_df_1['cleaned_text'] = control_df_1.text.apply(clean_up)"]},{"cell_type":"code","execution_count":null,"id":"df545e1d","metadata":{"id":"df545e1d","outputId":"4b09a476-56f3-42c6-b563-e3ea73b1e38d"},"outputs":[{"data":{"text/plain":["'hey! i one two co founders ideal, personal development strategy platform, eventual full fledged clothing brand! i experienced many hardships life offer. i born abusi household, branded social outcast school, depressed many years. however, my determination, sown my mind inspirational words my mentors, allowed me persevere my struggles. i challenged my inner demons, slowly conquering them one by one my daily quest self iprove. several years intense personal growth, i began college. i succeeded socially, morphing one men i aspired my younger years. nurturing potential others, watching them become something believed ipossible, my life long passion. month ago, i withdrew unirsity maryland assist you journey become ideal. feel free ask me anything you like! proof com a my story youtube. com watch? v amp t our website www. becomeideal. com\\nsocial media contact us info com'"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["#print(control_df_1.iloc[5678]['text'])\n","#control_df_1.iloc[5678]['cleaned_text']"]},{"cell_type":"markdown","id":"3f4e2d33","metadata":{"id":"3f4e2d33"},"source":["#### Here I have built this function in order to rescue the ***I*** which were masked and attcahed to the verb."]},{"cell_type":"code","execution_count":null,"id":"7f95f32a","metadata":{"id":"7f95f32a"},"outputs":[],"source":["def rescue_i(testo):\n","    testo = re.sub(r'im', 'i', testo)\n","    testo = re.sub(r'ive', 'i', testo)\n","    \n","    return testo"]},{"cell_type":"code","execution_count":null,"id":"29889982","metadata":{"id":"29889982"},"outputs":[],"source":["#apply the function to both datasets \n","control_df_1['cleaned_text'] = control_df_1.cleaned_text.apply(rescue_i)\n","cleaned_intero_tozip['cleaned_text'] = cleaned_intero_tozip.text.apply(rescue_i)"]},{"cell_type":"code","execution_count":null,"id":"fa619d09","metadata":{"id":"fa619d09"},"outputs":[],"source":["#print(control_df_1.iloc[5675]['cleaned_text'])\n","#print(cleaned_intero_tozip.iloc[5677]['cleaned_text'])"]},{"cell_type":"markdown","id":"a83ba563","metadata":{"id":"a83ba563"},"source":["#### Here below I have implemented the function which perform the tokenization\n","It's built thanks to other two function, a word tokenizer and a sentence tokenizer.\n","Moreover, I defined other two functions, one able to save my tokenized posts into a ***zip_file***, the other able to unzip the stored file and retrieve the tokenized posts. "]},{"cell_type":"code","execution_count":null,"id":"57ddfb7e","metadata":{"id":"57ddfb7e"},"outputs":[],"source":["def sent_tokenizer(text):\n","    return re.findall(r\".*?[.!\\?]\",text)\n","\n","#_____________________________________________________________\n","\n","def word_tokenizer(sentence):\n","    punct = r\"\"\"([A-z])([,;:\\?!\\.\"'])\"\"\"\n","    temp_sentence =  re.sub(punct, r\"\\1 \\2\", sentence)\n","    toks = temp_sentence.split()\n","    temp_out =[]\n","    # splitting english possessive\n","    for tok in toks:\n","        if re.search(r\"([A-z]+)’s?$\", tok):\n","            temp_out.extend(re.sub(r\"([A-z]+)(’s?)$\", r\"\\1 \\2\", tok).split())\n","        else:\n","            temp_out.append(tok)\n","    return temp_out \n","\n","#_____________________________________________________________\n","\n","def my_tokenizer(text):\n","    import string \n","    punct = string.punctuation \n","    sentences = sent_tokenizer(text)\n","    tokenized_text = []\n","    for sent in sentences:\n","        if len(sent) > 1:\n","            tokens = word_tokenizer(sent)\n","            tokens_1 = [tok for tok in tokens if not tok in punct]\n","            tokenized_text.append(tokens_1)\n","    return tokenized_text\n"]},{"cell_type":"code","execution_count":null,"id":"aabf454e","metadata":{"id":"aabf454e"},"outputs":[],"source":["def text2zip(path, lista_posts):\n","    \n","    arch_out = ZipFile(path, \"w\")\n","    out = []\n","    for i,article in enumerate(lista_posts):\n","        tok_sent = my_tokenizer(article)\n","        if len(tok_sent) > 2 and len(tok_sent[0]) > 1:\n","            \n","            out.append(tok_sent)\n","            text_article = \"\\n\".join([\" * \".join(tokens) for tokens in tok_sent]) # dividing tokens with \" * \"\n","            #print(text_article)\n","            arch_out.writestr(f\"{i}_{tok_sent[0][0]}.txt\", text_article)\n","    arch_out.close()\n","    return out\n","\n","#_____________________________________________________________\n","\n","def zip2text(path): \n","    arch_in = ZipFile(mypath, \"r\")\n","    files_name = arch_in.namelist()\n","    out = {}\n","    for file in files_name:\n","        text = arch_in.read(file).decode('utf-8')\n","        article = [sent.split(\" * \") for sent in text.split(\"\\n\")]\n","        out[file] = article\n","    arch_in.close()\n","    return out"]},{"cell_type":"code","execution_count":null,"id":"49cc84fd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49cc84fd","executionInfo":{"status":"ok","timestamp":1643878655758,"user_tz":-60,"elapsed":352,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"f143eec5-125b-4a93-db0c-b0a941f5d09a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Feeling * a * bit * depressedI * 've * been * in * a * big * low * all * weekend\n","I * don * 't * know * why * death * has * actually * crossed * my * mind * a * few * ties\n","I * don * 't * care * to * admit * it * to * any * of * my * friend * because * I * feel * disconnected * from * everyone\n","I * feel * so * lonely * yet * I * feel * so * overwhelmed * that * I * can * 't * really * talk * to * anyone\n","I * 've * been * pretty * isolated * for * the * past * few * days * mostly * doing * work\n","I * just * want * someone * to * talk * to * it * doesn * 't * have * to * be * about * depression\n","I * 'm * a * male * college * student * about * to * graduate\n","Was * going * to * hang * myself * but * didn * 't * have * guts * enough * to * kick * away * the * chairI * was * all * set * to * hang * myself * this * morning\n","Had * the * noose * around * my * neck * was * standing * on * the * chair * the * whole * nine * yards\n","I * just * couldn * 't * get * the * guts * to * kick * away * the * chair\n","Feel * more * down * and * hopeless * now * than * ever\n","Have * tried * to * kill * myself * many * ties * before * mainly * with * drug * overdoses\n","Not * going * to * go * that * route * though * is * that * it * 'll * likely * just * result * in * a * long * hospitalization * against * my * will\n","Life * has * become * intolerable * and * I * don * 't * want * to * be * here * anymore\n","Don * 't * see * any * other * way * out\n","Have * you * ever * maintained * a * poor * friendship * just * to * keep * the * last * friend * that * you * have\n","My * friend * and * I * grew * close * by * necessity\n","After * high * school * most * of * our * friends * moved * away * for * school * but * we * stuck * around\n","That * was * 5 * years * ago\n","Now * i * 'm * finding * that * she * is * changing * in * ways * that * are * really * frustrating * to * me * and * becoming * a * person * i * would * never * choose * to * be * friends * with\n","The * problem * is * she * is * literally * the * only * person * i * have * to * talk * to\n","Shes * the * only * one * that * knows * of * the * depression * relationships * and * other * things * I * 've * been * dealing * with\n","I * 'm * in * a * somewhat * new * relationship * (going * on * 6 * months * but * moving * very * slow).\n","I * would * have * somebody * to * lean * on * a * little * but * we * aren * 't * at * that * point * in * our * relationship * yet\n","I * 've * been * in * a * point * like * this * before * a * few * years * ago * I * had * a * really * close * friend * but * as * I * grew * up * and * matured * she * did * not * AT * ALL\n","I * found * it * ipossible * to * be * around * her * and * ended * that * relationship * very * suddenly\n","My * overall * mood * iproved * and * there * was * way * less * drama * in * my * life\n","I * 'm * worried * that * ending * this * friendship * won * 't * be * as * beneficial * as * it * was * to * end * the * one * a * few * years * ago\n","I * 'm * worried * it * will * be * more * detriental * to * me * than * beneficial * this * tie * around\n","You * ever * been * here\n","What * do\n","I * haven * 't * felt * positi * feelings * in * a * long * tie * I * don * 't * know * how * to * train * myself * to * do * that\n","I * know * happiness * is * a * choice\n","I * know * I * have * gin * up * too * much * of * my * life * to * being * sad\n","Because * my * current * self * is * wishing * he * could * go * back * and * slap * the * stupid * sadness * out * of * my * old * self * my * future * self * is * probably * doing * the * same * to * me * now\n","I * need * to * \"snap * out * of * it * but * I * don * 't * know * how * I * need * to * \"train * myself * to * be * happier * but * I * don * 't * know * how * it * 's * all * going * by * too * fast * and * I * don * 't * know * how * to * start * actually * living\n","Partners * of * those * who * suffer * from * depression * How * did * you * work * through * it\n","My * long * term * girlfriend * has * been * suffering * very * badly * from * depression * (medication * overdoses * self-harm * attempted * suicide * large * amounts * of * tie * in * psychiatriac * wards).\n","We * were * both * fine * up * until * a * month * or * two * ago\n","She * began * to * push * me * away * in * a * unemmotional * way * (as * in * she * showed * no * emmotion * in * telling * me * to * leave * her * alone).\n","The * change * in * her * towards * me * is * shocking * to * see * for * anyone * so * I * guess * I * was * just * going * to * ask * the * partners * out * there * if * they * have * any * advice\n","Should * I * be * helping * her * or * moving * on * like * she * wants\n","How * did * you * guys * get * through * it * as * the * partner\n","I * 'm * worthless\n","I * 've * gotten * whinier * and * weaker * and * needier * lately\n","I * 'm * lazy * and * useless * and * I * feel * like * I * just * dri * everyone * up * the * walls\n","The * world * would * be * better * off * if * I * were * gone\n","It * hurts * to * be * this * way\n","Its * like * I * can * 't * function * properly * and * I * haven * 't * even * entered * the * \"real * world\n","Someties * I * wonder * if * I * 'm * even * going * to * make * it * to * graduation\n","I * might * drop * out * or * end * up * killing * myself\n","I * can * 't * go * to * the * hospital * because * of * COVID-19.\n","Last * week * was * the * annirsary * of * my * brother * 's * death * and * its * still * taking * a * toll * on * me\n","I * just * don * 't * want * to * feel * anymore\n","What * 's * the * best * way * to * say * 'Goodbye * '?\n","There * 's * a * few * people * I * truly * do * love * and * care * for\n","They * put * some * tie * into * trying * to * help * me * and * I * don * 't * want * them * to * hurt * like * I * do\n","They * wasted * their * tie * on * me * I * 'm * sorry * to * say * it\n","I * want * to * apologize * to * them * before * I * do * it\n","\n","Help * with * Nightmares/night * terrors\n","Its * bad * enough * that * most * of * the * daytie * is * torture * but * now * i * no * longer * find * relief * in * sleeping * in * my * own * bed\n","Every * night * my * nightmares * get * worse * and * scarier/more * intense\n","Then * i * wake * up * all * stressed * out * and * stay * that * way * all * through * the * morning\n","Does * anyone * else * experience * this\n","Or * does * anyone * have * advice\n","My * depression * has * lasted * over * a * month * and * it * doesn * 't * seem * to * be * getting * better\n","I * 'm * not * sure * where * to * go * with * this * so * i * 'll * start * with * this * [AMA](http * ://www\n","reddit\n","com/r/IAmA/comments/fvyjk/i_am_somebody_who_fell_in_love_with_someone_my/) * I * did * 4 * months * ago\n","Since * then * my * mood * was * pretty * high\n","It * felt * good * to * get * shit * off * my * chest * especially * something * i * bottled * up * for * so * long\n","It * also * felt * good * knowing * a * friend * of * mine * stumbled * onto * the * AMA * realized * it * was * me * and * didn * 't * treat * me * any * differently\n","That * felt * really * good * if * a * bit * embarrassing * at * first\n","But * for * the * last * month * all * I * can * really * think * of * is * my * ex * (real * ex).\n","It * 's * been * almost * 6 * months * and * i * just * can * 't * stop * thinking * of * her\n","It * doesn * 't * help * that * I * run * into * her * sister * all * over * the * place * (She * lis * just * around * the * corner * from * my * new * place * which * doesn * 't * help).\n","It * 's * gotten * to * a * point * where * about * two * weeks * ago * I * bought * a * thick * rope * and * bookmarked * [this](http * ://www\n","wikihow\n","com/Tie-a-Hangmans-Noose) * page\n","Obviously * as * I * 'm * typing * this * post * I * haven * 't * done * anything * with * the * rope * etc * but * it * 's * always * on * the * back * of * my * mind\n","I * try * to * go * out * each * day * hang * with * people * and * get * my * mind * off * things * but * it * feels * like * I * 'm * just * going * through * the * motions\n","It * doesn * 't * feel * like * I * 'm * living * but * just * existing\n","I * have * tried * going * to * the * hospital * and * speaking * with * a * psychiatrist * but * they * just * pat * me * on * the * head * and * tell * me * my * feelings * are * normal * and * that * they * will * pass\n","How * much * further * do * I * need * to * go * before * they * will * take * me * seriously\n","Do * I * need * to * have * a * rope * mark * around * my * neck * before * they * will * do * anything\n","edit * Probably * shouldn * 't * have * posted * this * so * close * to * bed * tie\n","I * 'm * going * to * bed * now * but * i * 'll * respond * to * any * questions * in * the * morning\n"]}],"source":["#trying to visualize the text once it has been cleaned; an asterisk has been put between each token just to have a better visualization\n","mental_illness = cleaned_intero_tozip['cleaned_text'][:10]\n","out = []\n","for i,article in enumerate(mental_illness):\n","    tok_sent = my_tokenizer(article)\n","    out.append(tok_sent)\n","    text_article = \"\\n\".join([\" * \".join(tokens) for tokens in tok_sent])\n","    print(text_article)"]},{"cell_type":"code","execution_count":null,"id":"68e9b6fb","metadata":{"id":"68e9b6fb"},"outputs":[],"source":["control_group = control_df_1['cleaned_text'][:]\n","post_control = list(control_group)"]},{"cell_type":"code","execution_count":null,"id":"d02ab2b8","metadata":{"id":"d02ab2b8"},"outputs":[],"source":["#I save the archive file with posts related to control group\n","mypath = \"/content/drive/MyDrive/Computational_Linguistics_Project /myarchive_control.zip\" \n","lista_1 = text2zip(mypath,post_control)"]},{"cell_type":"code","execution_count":null,"id":"6a9f8e76","metadata":{"id":"6a9f8e76","outputId":"dbc4a387-3687-442a-953d-742dcf98e720"},"outputs":[{"data":{"text/plain":["[['i',\n","  'author',\n","  'two',\n","  'novels',\n","  'dead',\n","  'cats',\n","  'reflections',\n","  'parenthood',\n","  'anarchy',\n","  'lies'],\n"," ['i',\n","  'writing',\n","  'dystopian',\n","  'novels',\n","  'dystopia',\n","  'publishing',\n","  'world',\n","  'going',\n","  'end',\n","  'i',\n","  'thoughts'],\n"," ['check', 'my', 'books', 'short', 'stories', 'here', 'jessemckinnell'],\n"," ['com', 'proof', 'redd'],\n"," ['it']]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["#just checking\n","lista_1[800][:]"]},{"cell_type":"code","execution_count":null,"id":"50879ced","metadata":{"id":"50879ced"},"outputs":[],"source":["mental_illness = cleaned_intero_tozip['cleaned_text'][:]\n","post_illness = list(mental_illness)"]},{"cell_type":"code","execution_count":null,"id":"da26c7a6","metadata":{"id":"da26c7a6"},"outputs":[],"source":["#I save the archive file with posts related to mental illness\n","mypath_2 = \"/content/drive/MyDrive/Computational_Linguistics_Project /myarchive_illness.zip\" \n","lista_2 = text2zip(mypath_2,post_illness)"]},{"cell_type":"code","execution_count":null,"id":"f725818c","metadata":{"id":"f725818c"},"outputs":[],"source":["control_df_1.to_csv('FINALE_controllo.csv')"]},{"cell_type":"code","execution_count":null,"id":"4a4e1086","metadata":{"id":"4a4e1086"},"outputs":[],"source":["experimental_df_1.to_csv('FINALE_sperimentale.csv')"]},{"cell_type":"markdown","id":"b0de3938","metadata":{"id":"b0de3938"},"source":["### Here I split the experimental group into two "]},{"cell_type":"code","execution_count":null,"id":"05316130","metadata":{"id":"05316130"},"outputs":[],"source":["depression = experimental_df_1[experimental_df_1['class'] == 'depression']"]},{"cell_type":"code","execution_count":null,"id":"b2052cfd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2052cfd","executionInfo":{"status":"ok","timestamp":1643879087812,"user_tz":-60,"elapsed":340033,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"0704aa0d-1081-490d-f07f-00f295efb75b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["depression['cleaned_text'] = depression.text.apply(clean_up)"]},{"cell_type":"code","execution_count":null,"id":"d80a2c06","metadata":{"id":"d80a2c06"},"outputs":[],"source":["depression.to_csv('depression_finale.csv')"]},{"cell_type":"code","execution_count":null,"id":"33470f66","metadata":{"id":"33470f66"},"outputs":[],"source":["suicide = experimental_df_1[experimental_df_1['class'] == 'SuicideWatch']"]},{"cell_type":"code","execution_count":null,"id":"6e8edd8f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6e8edd8f","executionInfo":{"status":"ok","timestamp":1643879340486,"user_tz":-60,"elapsed":195134,"user":{"displayName":"Francesca Padovani","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831289911927475598"}},"outputId":"513f2c0f-3e84-40d5-a754-7dfc4debff69"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["suicide['cleaned_text'] = suicide.text.apply(clean_up)"]},{"cell_type":"code","execution_count":null,"id":"40f37d6b","metadata":{"id":"40f37d6b"},"outputs":[],"source":["suicide.to_csv('suicidio_finale.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"2_Build_corpus_mental_health.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}